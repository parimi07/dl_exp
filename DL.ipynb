{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Implement Logistic regression With Neural Network Mindset.\n",
    "#  ● logistic regression classifier for classification. \n",
    "#  ● Plot the loss over each epoch.\n",
    "#  ● Plot the accuracy over each epoch\n",
    "# ● Report final Accuracy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Derivative of activation functions\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return (a > 0).astype(float)\n",
    "\n",
    "# Loss functions\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -1/m * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return np.sum((y_true - y_pred) ** 2) / m\n",
    "\n",
    "# Derivative of loss functions\n",
    "def binary_cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "def mean_squared_error_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "# Prompt user to select activation and loss functions\n",
    "print(\"Select activation function:\")\n",
    "print(\"1. Sigmoid\")\n",
    "print(\"2. ReLU\")\n",
    "activation_choice = input(\"Enter choice (1/2): \")\n",
    "\n",
    "print(\"Select loss function:\")\n",
    "print(\"1. Binary Cross-Entropy\")\n",
    "print(\"2. Mean Squared Error\")\n",
    "loss_choice = input(\"Enter choice (1/2): \")\n",
    "\n",
    "# Set activation and loss functions based on user choice\n",
    "if activation_choice == \"1\":\n",
    "    activation_fn = sigmoid\n",
    "    activation_derivative_fn = sigmoid_derivative\n",
    "elif activation_choice == \"2\":\n",
    "    activation_fn = relu\n",
    "    activation_derivative_fn = relu_derivative\n",
    "else:\n",
    "    raise ValueError(\"Invalid activation function choice\")\n",
    "\n",
    "if loss_choice == \"1\":\n",
    "    loss_fn = binary_cross_entropy\n",
    "    loss_derivative_fn = binary_cross_entropy_derivative\n",
    "elif loss_choice == \"2\":\n",
    "    loss_fn = mean_squared_error\n",
    "    loss_derivative_fn = mean_squared_error_derivative\n",
    "else:\n",
    "    raise ValueError(\"Invalid loss function choice\")\n",
    "\n",
    "# Generate a binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, random_state=1)\n",
    "y = y.reshape(y.shape[0], 1)  # Reshape y to be a column vector\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Function to train the logistic regression model\n",
    "def train(X, y, learning_rate=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    W = np.zeros((n, 1))\n",
    "    b = 0\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        Z = np.dot(X, W) + b\n",
    "        A = activation_fn(Z)\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        loss = loss_fn(y, A)\n",
    "        losses.append(loss)\n",
    "        predictions = A > 0.5\n",
    "        acc = accuracy_score(y, predictions)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        # Backward pass (gradient descent)\n",
    "        dZ = loss_derivative_fn(y, A) * activation_derivative_fn(A)\n",
    "        dW = (1/m) * np.dot(X.T, dZ)\n",
    "        db = (1/m) * np.sum(dZ)\n",
    "\n",
    "        # Update weights and bias\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "    return W, b, losses, accuracies\n",
    "\n",
    "# Train the model and get the loss and accuracy over epochs\n",
    "W, b, losses, accuracies = train(X_train, y_train, learning_rate=0.01, epochs=1000)\n",
    "\n",
    "# Plot loss and accuracy over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, label=\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "Z_test = np.dot(X_test, W) + b\n",
    "A_test = activation_fn(Z_test)\n",
    "y_pred_test = A_test > 0.5\n",
    "final_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Final Test Accuracy:\", final_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Implement Shallow Neural Network model:\n",
    "#  ● Implement a binary classifi cation neural network with a single and multiple hidden layers.\n",
    "#  ● Implement a Multi-class classifi cation neural network with a single and multiple hidden \n",
    "# layers.\n",
    "#  ● Vary the number of neurons at suitable layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return np.where(a > 0, 1, 0)\n",
    "\n",
    "def tanh_derivative(a):\n",
    "    return 1 - np.square(a)\n",
    "\n",
    "# Loss functions\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -1/m * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -1/m * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Neural network model class\n",
    "class ShallowNeuralNetwork:\n",
    "    def __init__(self, layer_dims, output_activation='sigmoid', hidden_activation='relu', loss='binary_cross_entropy'):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.output_activation = output_activation\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.loss = loss\n",
    "        self.params = self.initialize_params()\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    def initialize_params(self):\n",
    "        np.random.seed(1)\n",
    "        params = {}\n",
    "        for l in range(1, len(self.layer_dims)):\n",
    "            params[f\"W{l}\"] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * 0.01\n",
    "            params[f\"b{l}\"] = np.zeros((self.layer_dims[l], 1))\n",
    "        return params\n",
    "\n",
    "    # Forward pass\n",
    "    def forward_propagation(self, X):\n",
    "        A = X\n",
    "        cache = {\"A0\": A}\n",
    "        L = len(self.layer_dims) - 1\n",
    "\n",
    "        # Hidden layers\n",
    "        for l in range(1, L):\n",
    "            Z = np.dot(self.params[f\"W{l}\"], A) + self.params[f\"b{l}\"]\n",
    "            if self.hidden_activation == 'sigmoid':\n",
    "                A = sigmoid(Z)\n",
    "            elif self.hidden_activation == 'relu':\n",
    "                A = relu(Z)\n",
    "            elif self.hidden_activation == 'tanh':\n",
    "                A = tanh(Z)\n",
    "            cache[f\"A{l}\"], cache[f\"Z{l}\"] = A, Z\n",
    "\n",
    "        # Output layer\n",
    "        ZL = np.dot(self.params[f\"W{L}\"], A) + self.params[f\"b{L}\"]\n",
    "        if self.output_activation == \"sigmoid\":\n",
    "            AL = sigmoid(ZL)\n",
    "        elif self.output_activation == \"softmax\":\n",
    "            AL = softmax(ZL)\n",
    "        elif self.output_activation == \"relu\":\n",
    "            AL = relu(ZL)\n",
    "        cache[f\"A{L}\"], cache[f\"Z{L}\"] = AL, ZL\n",
    "        return AL, cache\n",
    "\n",
    "    # Backward pass\n",
    "    def backward_propagation(self, X, y, cache):\n",
    "        m = X.shape[1]\n",
    "        grads = {}\n",
    "        L = len(self.layer_dims) - 1\n",
    "        AL = cache[f\"A{L}\"]\n",
    "\n",
    "        # Loss function derivatives\n",
    "        if self.loss == 'binary_cross_entropy':\n",
    "            dZL = AL - y\n",
    "        elif self.loss == 'categorical_cross_entropy':\n",
    "            dZL = AL - y\n",
    "        elif self.loss == 'mean_squared_error':\n",
    "            dZL = AL - y\n",
    "\n",
    "        # Output layer gradients\n",
    "        grads[f\"dW{L}\"] = (1/m) * np.dot(dZL, cache[f\"A{L-1}\"].T)\n",
    "        grads[f\"db{L}\"] = (1/m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "\n",
    "        # Hidden layers gradients\n",
    "        for l in reversed(range(1, L)):\n",
    "            dA = np.dot(self.params[f\"W{l+1}\"].T, dZL)\n",
    "            if self.hidden_activation == 'sigmoid':\n",
    "                dZ = dA * sigmoid_derivative(cache[f\"A{l}\"])\n",
    "            elif self.hidden_activation == 'relu':\n",
    "                dZ = dA * relu_derivative(cache[f\"A{l}\"])\n",
    "            elif self.hidden_activation == 'tanh':\n",
    "                dZ = dA * tanh_derivative(cache[f\"A{l}\"])\n",
    "            grads[f\"dW{l}\"] = (1/m) * np.dot(dZ, cache[f\"A{l-1}\"].T)\n",
    "            grads[f\"db{l}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            dZL = dZ\n",
    "\n",
    "        return grads\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, grads, learning_rate):\n",
    "        for l in range(1, len(self.layer_dims)):\n",
    "            self.params[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "            self.params[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "\n",
    "    # Train model\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            AL, cache = self.forward_propagation(X)\n",
    "            if self.loss == \"binary_cross_entropy\":\n",
    "                loss = binary_cross_entropy(y, AL)\n",
    "            elif self.loss == \"categorical_cross_entropy\":\n",
    "                loss = categorical_cross_entropy(y, AL)\n",
    "            elif self.loss == \"mean_squared_error\":\n",
    "                loss = mean_squared_error(y, AL)\n",
    "            losses.append(loss)\n",
    "            grads = self.backward_propagation(X, y, cache)\n",
    "            self.update_params(grads, learning_rate)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        AL, _ = self.forward_propagation(X)\n",
    "        if self.output_activation == \"sigmoid\":\n",
    "            return AL > 0.5\n",
    "        elif self.output_activation == \"softmax\":\n",
    "            return np.argmax(AL, axis=0)\n",
    "\n",
    "# Prepare dataset\n",
    "def prepare_data(binary=True):\n",
    "    if binary:\n",
    "        X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, random_state=1)\n",
    "        y = y.reshape(y.shape[0], 1)\n",
    "        output_activation = \"sigmoid\"\n",
    "        loss = \"binary_cross_entropy\"\n",
    "    else:\n",
    "        X, y = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=1)\n",
    "        y = y.reshape(y.shape[0], 1)\n",
    "        enc = OneHotEncoder(sparse=False)\n",
    "        y = enc.fit_transform(y)\n",
    "        output_activation = \"softmax\"\n",
    "        loss = \"categorical_cross_entropy\"\n",
    "    return X, y, output_activation, loss\n",
    "\n",
    "# Binary Classification Example\n",
    "X, y, output_activation, loss = prepare_data(binary=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.T, y.T, test_size=0.2, random_state=1)\n",
    "layer_dims = [X_train.shape[0], 5, 1]\n",
    "model = ShallowNeuralNetwork(layer_dims, output_activation=output_activation, hidden_activation='relu', loss=loss)\n",
    "losses = model.train(X_train, y_train, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.show()\n",
    "\n",
    "# Predict and Accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test.T, y_pred.T)\n",
    "print(f\"Final Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Hyper parameter Tuning of a Neural Network model implemented for hand-written digit \n",
    "# classifi cation:\n",
    "#  ● Vary the type of activation functions.\n",
    "#  ● Choose suitable Loss functions.\n",
    "#  ● Vary the number of neurons at suitable layers.\n",
    "#  ● Vary Weight Initialization methods.\n",
    "#  ● Save the Best Model and load the saved model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'activation': ['relu', 'sigmoid', 'tanh'],\n",
    "    'loss': ['categorical_crossentropy', 'mean_squared_error'],\n",
    "    'neurons': [64, 128],\n",
    "    'initializer': ['he_uniform', 'glorot_uniform', 'random_normal']\n",
    "}\n",
    "\n",
    "# Placeholder for best model and its score\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing with parameters: {params}\")\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(params['neurons'], activation=params['activation'], kernel_initializer=params['initializer']),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(), loss=params['loss'], metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks: save the model if it has the best accuracy\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32, callbacks=[checkpoint, early_stop], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss, val_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    # Update best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest Model Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.h5')\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy of Best Model: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Building a Deep Neural Network:  \n",
    "# ● Implement a multi-class classifi cation neural network with number of layers of your choice.\n",
    "#  ● Include Batch Normalization layers.\n",
    "#  ● Vary Optimization methods.\n",
    "#  ● Add drop out layers.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define a grid of hyperparameters to vary optimizers and dropout rates\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(), SGD(), RMSprop()],\n",
    "    'dropout_rate': [0.2, 0.4],\n",
    "    'batch_norm': [True, False]\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Loop over hyperparameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nTesting with parameters: {params}\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    # Add layers with batch normalization and dropout\n",
    "    for _ in range(3):  # Using 3 hidden layers as an example\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        if params['batch_norm']:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))  # Output layer\n",
    "    \n",
    "    # Compile model with chosen optimizer\n",
    "    model.compile(optimizer=params['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks for saving the best model\n",
    "    checkpoint = ModelCheckpoint('best_deep_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32, callbacks=[checkpoint, early_stop], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss, val_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    # Track the best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "# Output the best parameters and accuracy\n",
    "print(f\"\\nBest Model Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Load and evaluate the best saved model\n",
    "best_model = tf.keras.models.load_model('best_deep_model.h5')\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy of Best Model: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Convolutional Neural Network Models.\n",
    "#  ● Design a Convolutional neural network with the layers of your choice\n",
    "#  ● Compare the performance by changing the \n",
    "# ● Kernel size\n",
    "#  ● Number of feature maps at each convolutional layer\n",
    "#  ● Stride.\n",
    "#  ● Padding.\n",
    "#  ● Number of fully connected layers.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0  # Reshape and normalize\n",
    "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'kernel_size': [(3, 3), (5, 5)],\n",
    "    'feature_maps': [32, 64],\n",
    "    'stride': [1, 2],\n",
    "    'padding': ['same', 'valid'],\n",
    "    'fully_connected_layers': [1, 2]\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nTesting with parameters: {params}\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(Conv2D(filters=params['feature_maps'],\n",
    "                     kernel_size=params['kernel_size'],\n",
    "                     strides=params['stride'],\n",
    "                     padding=params['padding'],\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Second convolutional layer with doubled filters\n",
    "    model.add(Conv2D(filters=params['feature_maps'] * 2,\n",
    "                     kernel_size=params['kernel_size'],\n",
    "                     strides=params['stride'],\n",
    "                     padding=params['padding'],\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add fully connected layers\n",
    "    for _ in range(params['fully_connected_layers']):\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define checkpoint to save the best model\n",
    "    checkpoint = ModelCheckpoint('best_cnn_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=5, validation_split=0.2, batch_size=32, callbacks=[checkpoint], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss, val_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    # Track the best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "# Output the best parameters and accuracy\n",
    "print(f\"\\nBest Model Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Load and evaluate the best saved model\n",
    "best_model = tf.keras.models.load_model('best_cnn_model.h5')\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy of Best Model: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization of CNN Models.\n",
    "#  ● Design a Convolutional Neural Network Model for image classifi cation.\n",
    "#  ● Plot Model Architecture.\n",
    "#  ● Visualize feature maps after training of CNN.\n",
    "#  ● Visualize class activation maps\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0  # Reshape and normalize\n",
    "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Step 1: Design and compile a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.1, batch_size=32)\n",
    "\n",
    "# Plot the model architecture\n",
    "tf.keras.utils.plot_model(model, to_file=\"model_architecture.png\", show_shapes=True)\n",
    "\n",
    "# Step 2: Visualize Feature Maps for a Sample Image\n",
    "def plot_feature_maps(model, layer_indices, input_image):\n",
    "    for layer_index in layer_indices:\n",
    "        layer_model = Model(inputs=model.input, outputs=model.layers[layer_index].output)\n",
    "        feature_maps = layer_model.predict(input_image)\n",
    "        \n",
    "        # Plot each feature map\n",
    "        num_feature_maps = feature_maps.shape[-1]\n",
    "        fig, axes = plt.subplots(1, num_feature_maps, figsize=(15, 15))\n",
    "        for i in range(num_feature_maps):\n",
    "            axes[i].imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Select a sample image\n",
    "sample_image = X_test[0].reshape(1, 28, 28, 1)\n",
    "\n",
    "# Visualize feature maps for the first and second convolutional layers\n",
    "plot_feature_maps(model, layer_indices=[0, 2], input_image=sample_image)\n",
    "\n",
    "# Step 3: Visualize Class Activation Maps (CAMs)\n",
    "def generate_cam(model, input_image, class_index):\n",
    "    # Create a model that outputs the feature maps from the last conv layer\n",
    "    last_conv_layer = model.get_layer(\"conv2d_1\")\n",
    "    grad_model = Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(input_image)\n",
    "        loss = predictions[:, class_index]\n",
    "\n",
    "    # Compute gradients of the target class wrt the last conv layer output\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    # Weight the conv layer outputs by the pooled gradients\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    cam = np.zeros(conv_outputs.shape[:2], dtype=np.float32)\n",
    "    for i, w in enumerate(pooled_grads):\n",
    "        cam += w * conv_outputs[:, :, i]\n",
    "    \n",
    "    # Apply ReLU and normalize CAM\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "\n",
    "    # Resize to match input image dimensions and plot\n",
    "    cam = tf.image.resize(cam[..., tf.newaxis], (28, 28)).numpy()\n",
    "    plt.imshow(input_image[0].reshape(28, 28), cmap='gray')\n",
    "    plt.imshow(cam, cmap='jet', alpha=0.5)  # Overlay CAM with original image\n",
    "    plt.title(\"Class Activation Map (CAM)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate CAM for a specific class (e.g., the predicted class of the sample image)\n",
    "predicted_class = np.argmax(model.predict(sample_image))\n",
    "generate_cam(model, sample_image, predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Using Deep pre-trained CNN model for feature extraction:\n",
    "#  ● Extract features from the FC1 of VGG network.\n",
    "#  ● Train any traditional ML model like SVM for classifi cation. \n",
    "# ● Repeat the above by considering FC2 of VGG for feature extraction.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Step 1: Load VGG16 or VGG19 pre-trained model without the top layer (to remove FC layers)\n",
    "vgg_model = VGG16(weights='imagenet', include_top=True)\n",
    "# If using VGG19:\n",
    "# vgg_model = VGG19(weights='imagenet', include_top=True)\n",
    "\n",
    "# Step 2: Create new models to extract features from FC1 and FC2 layers\n",
    "# FC1 Layer - Flatten layer after the fully connected 1 layer\n",
    "fc1_model = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('fc1').output)\n",
    "\n",
    "# FC2 Layer - Flatten layer after the fully connected 2 layer\n",
    "fc2_model = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('fc2').output)\n",
    "\n",
    "# Function to preprocess an image and extract features from the specified layer\n",
    "def extract_features(img_path, model):\n",
    "    img = load_img(img_path, target_size=(224, 224))  # VGG input size: 224x224\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.keras.applications.vgg16.preprocess_input(img_array)  # Preprocessing for VGG\n",
    "    features = model.predict(img_array)\n",
    "    return features.flatten()\n",
    "\n",
    "# Step 3: Load the dataset\n",
    "# Let's assume you have a folder with images for each class (e.g., 'class1', 'class2')\n",
    "# For simplicity, assuming folder structure:\n",
    "# - data/\n",
    "#   - class1/\n",
    "#   - class2/\n",
    "#   - ...\n",
    "dataset_path = '/path/to/dataset/'\n",
    "class_names = os.listdir(dataset_path)\n",
    "\n",
    "# Prepare lists to store features and labels\n",
    "features_fc1 = []\n",
    "features_fc2 = []\n",
    "labels = []\n",
    "\n",
    "# Loop through all the classes and extract features\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        \n",
    "        # Extract features from FC1 and FC2 layers\n",
    "        fc1_features = extract_features(img_path, fc1_model)\n",
    "        fc2_features = extract_features(img_path, fc2_model)\n",
    "        \n",
    "        features_fc1.append(fc1_features)\n",
    "        features_fc2.append(fc2_features)\n",
    "        labels.append(label)\n",
    "\n",
    "# Step 4: Convert lists to numpy arrays\n",
    "features_fc1 = np.array(features_fc1)\n",
    "features_fc2 = np.array(features_fc2)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Step 5: Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_fc1_scaled = scaler.fit_transform(features_fc1)\n",
    "features_fc2_scaled = scaler.fit_transform(features_fc2)\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train_fc1, X_test_fc1, y_train, y_test = train_test_split(features_fc1_scaled, labels, test_size=0.2, random_state=42)\n",
    "X_train_fc2, X_test_fc2 = train_test_split(features_fc2_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train an SVM classifier on the features extracted from FC1\n",
    "svm_fc1 = SVC(kernel='linear')  # You can experiment with different SVM kernels\n",
    "svm_fc1.fit(X_train_fc1, y_train)\n",
    "\n",
    "# Step 8: Evaluate the SVM model for FC1\n",
    "y_pred_fc1 = svm_fc1.predict(X_test_fc1)\n",
    "print(\"Classification report for FC1 features:\")\n",
    "print(classification_report(y_test, y_pred_fc1))\n",
    "\n",
    "# Step 9: Train an SVM classifier on the features extracted from FC2\n",
    "svm_fc2 = SVC(kernel='linear')  # You can experiment with different SVM kernels\n",
    "svm_fc2.fit(X_train_fc2, y_train)\n",
    "\n",
    "# Step 10: Evaluate the SVM model for FC2\n",
    "y_pred_fc2 = svm_fc2.predict(X_test_fc2)\n",
    "print(\"Classification report for FC2 features:\")\n",
    "print(classification_report(y_test, y_pred_fc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Fine-tuning Deep pre-trained CNN for Classifi cation:\n",
    "#  ● Fine-tune VGG network for the task under consideration.\n",
    "#  ● Check the performance by making.\n",
    "#  ● all the layers trainable.\n",
    "#  ● freezing the initial layers.\n",
    "#  ● freezing the entire network except the fi nal layer.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load selected pre-trained model\n",
    "def load_pretrained_model(model_name, input_shape, num_classes):\n",
    "    if model_name == \"VGG16\":\n",
    "        base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif model_name == \"VGG19\":\n",
    "        base_model = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif model_name == \"ResNet50\":\n",
    "        base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif model_name == \"AlexNet\":\n",
    "        base_model = create_alexnet(input_shape)\n",
    "    else:\n",
    "        print(\"Invalid model name selected.\")\n",
    "        return None\n",
    "    \n",
    "    # Add custom top layers for classification\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Function to create AlexNet\n",
    "def create_alexnet(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv2D(96, (11, 11), strides=4, padding='valid', activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "        Conv2D(256, (5, 5), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "        Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1000, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Display available models and prompt user to select one\n",
    "print(\"Available Models: [VGG16, VGG19, ResNet50, AlexNet]\")\n",
    "selected_model = input(\"Please enter the model you want to use: \")\n",
    "\n",
    "# Set up data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, horizontal_flip=True, rotation_range=15)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'path_to_train_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'path_to_validation_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load the selected model\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = train_generator.num_classes\n",
    "model = load_pretrained_model(selected_model, input_shape, num_classes)\n",
    "\n",
    "if model:\n",
    "    # Helper function to compile and train the model\n",
    "    def compile_and_train(model, trainable_layers, epochs=5):\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = layer.name in trainable_layers\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    # Fine-tuning strategy examples\n",
    "    print(\"Training all layers...\")\n",
    "    all_layers = [layer.name for layer in model.layers]\n",
    "    history_all_trainable = compile_and_train(model, all_layers)\n",
    "\n",
    "    print(\"Training with frozen initial layers...\")\n",
    "    initial_frozen_layers = [layer.name for layer in model.layers[10:]]\n",
    "    history_initial_frozen = compile_and_train(model, initial_frozen_layers)\n",
    "\n",
    "    print(\"Training with only the final layer trainable...\")\n",
    "    final_layer_only = [model.layers[-1].name]\n",
    "    history_final_layer_only = compile_and_train(model, final_layer_only)\n",
    "\n",
    "    # Plotting performance for each strategy\n",
    "    def plot_history(history, label):\n",
    "        plt.plot(history.history['accuracy'], label=f'{label} - Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label=f'{label} - Val Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    plot_history(history_all_trainable, \"All Layers Trainable\")\n",
    "    plot_history(history_initial_frozen, \"Initial Layers Frozen\")\n",
    "    plot_history(history_final_layer_only, \"Final Layer Only\")\n",
    "else:\n",
    "    print(\"No valid model selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Design MLFFNN with 3-level stacked autoencoder based pre-training for Black and white \n",
    "# image data, Display features extracted by diff erent levels of stacked autoencoder at the end \n",
    "# of pre-training.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset (black and white images)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Define the autoencoder structure\n",
    "def create_autoencoder(input_shape, encoding_dim):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    x = Flatten()(input_img)\n",
    "    x = Dense(encoding_dim, activation='relu')(x)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    x = Dense(np.prod(input_shape), activation='sigmoid')(x)\n",
    "    decoded = Reshape(input_shape)(x)\n",
    "    \n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    encoder = Model(input_img, encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Create 3 levels of stacked autoencoders\n",
    "input_shape = (28, 28, 1)\n",
    "encoding_dim1 = 128\n",
    "encoding_dim2 = 64\n",
    "encoding_dim3 = 32\n",
    "\n",
    "# Level 1 Autoencoder\n",
    "autoencoder1, encoder1 = create_autoencoder(input_shape, encoding_dim1)\n",
    "\n",
    "# Train first autoencoder\n",
    "autoencoder1.fit(x_train, x_train, epochs=5, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
    "\n",
    "# Extract features from the first autoencoder (encoded features)\n",
    "x_train_encoded1 = encoder1.predict(x_train)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "# Level 2 Autoencoder\n",
    "autoencoder2, encoder2 = create_autoencoder((encoding_dim1,), encoding_dim2)\n",
    "\n",
    "# Train second autoencoder\n",
    "autoencoder2.fit(x_train_encoded1, x_train_encoded1, epochs=5, batch_size=256, shuffle=True, validation_data=(x_test_encoded1, x_test_encoded1))\n",
    "\n",
    "# Extract features from the second autoencoder\n",
    "x_train_encoded2 = encoder2.predict(x_train_encoded1)\n",
    "x_test_encoded2 = encoder2.predict(x_test_encoded1)\n",
    "\n",
    "# Level 3 Autoencoder\n",
    "autoencoder3, encoder3 = create_autoencoder((encoding_dim2,), encoding_dim3)\n",
    "\n",
    "# Train third autoencoder\n",
    "autoencoder3.fit(x_train_encoded2, x_train_encoded2, epochs=5, batch_size=256, shuffle=True, validation_data=(x_test_encoded2, x_test_encoded2))\n",
    "\n",
    "# Extract features from the third autoencoder\n",
    "x_train_encoded3 = encoder3.predict(x_train_encoded2)\n",
    "x_test_encoded3 = encoder3.predict(x_test_encoded2)\n",
    "\n",
    "# Visualize the extracted features at each level of the stacked autoencoder\n",
    "def plot_features(features, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(features[0].reshape(1, -1), cmap='gray', aspect='auto')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Plot features at different levels\n",
    "plot_features(x_train_encoded1, \"Features extracted by first autoencoder\")\n",
    "plot_features(x_train_encoded2, \"Features extracted by second autoencoder\")\n",
    "plot_features(x_train_encoded3, \"Features extracted by third autoencoder\")\n",
    "\n",
    "# Building the MLFFNN for classification\n",
    "def build_mlffnn(input_dim, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(512, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Use the features extracted from the third autoencoder as input to the MLFFNN\n",
    "input_dim = x_train_encoded3.shape[1]\n",
    "mlffnn = build_mlffnn(input_dim, 10)  # 10 classes for MNIST\n",
    "\n",
    "# Fine-tune the MLFFNN on the features from the 3rd level of the stacked autoencoder\n",
    "mlffnn.fit(x_train_encoded3, y_train, epochs=10, batch_size=256, validation_data=(x_test_encoded3, y_test))\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "score = mlffnn.evaluate(x_test_encoded3, y_test)\n",
    "print(f\"Test Loss: {score[0]}, Test Accuracy: {score[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Sentiment Analysis\n",
    "#  ● Pre-process the text.\n",
    "#  ● Convert the text into word embeddings.\n",
    "#  ● Implement the classifi cation network using LSTMs/ GRUs.\n",
    "#  ● Pre-process the text.\n",
    "#  ● Convert the text into word embeddings.\n",
    "#  ● Implement the classifi cation network using LSTMs/ GRUs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "max_len = 200\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "def create_model(embedding_dim, max_len, use_gru=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=embedding_dim, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    if use_gru:\n",
    "        model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    else:\n",
    "        model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(embedding_dim, max_len, use_gru=False)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "score, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {score}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
